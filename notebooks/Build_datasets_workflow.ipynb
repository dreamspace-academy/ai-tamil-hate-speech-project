{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i_sDNxi1mSr"
      },
      "source": [
        "# Build datasets workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfnFw-1V1n6K"
      },
      "source": [
        "## Install necessary libraries for loading repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usbkfzv3FNkb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dvc fastds\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTN0bclB10OD"
      },
      "source": [
        "## Set all credentials and download all necessary files/data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjLbZTpg8ZTj"
      },
      "source": [
        "### Set up local repo and branch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_token = getpass('dagshub access token: ') \n",
        "your_token = urllib.parse.quote(your_token) \n",
        "\n",
        "your_username = input('dagshub username: ')\n",
        "your_email = input('email address: ')"
      ],
      "metadata": {
        "id": "q7TpveiIodZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ_mzvWzsdrO"
      },
      "outputs": [],
      "source": [
        "# Clone repo with personal token (Settings -> Tokens -> Default Access Token)\n",
        "cmd_string = 'git clone https://{0}@dagshub.com/Omdena/NYU.git'.format(your_token)\n",
        "os.system(cmd_string)\n",
        "%cd NYU\n",
        "\n",
        "# Switch to branch you want to work with and sync with remote branch (if necessary)\n",
        "!git fetch origin\n",
        "#!git checkout -b cross-validation origin/cross-validation\n",
        "\n",
        "# Change directory to training workflow\n",
        "%cd tasks/task-4-language-transformer-models/workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgaX4bX0CIAV"
      },
      "source": [
        "### Set up DVC and git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYZJTlVTn1Dp"
      },
      "outputs": [],
      "source": [
        "!dvc remote modify --local origin auth basic\n",
        "!dvc remote modify --local origin user '{your_username}'\n",
        "!dvc remote modify --local origin password '{your_token}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIpfRrxpoJ2X"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email '{your_email}'\n",
        "!git config --global user.name '{your_username}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Eqzw9lCTCO"
      },
      "source": [
        "### Pull training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IidgFZzjqLsJ"
      },
      "outputs": [],
      "source": [
        "!dvc pull -r origin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6qcnPwVzqOw"
      },
      "source": [
        "## Data processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTIkOjefakZt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DreamSpace increments for positive examples"
      ],
      "metadata": {
        "id": "dxGcm7d-AOMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjgRjP2YMYwR"
      },
      "outputs": [],
      "source": [
        "path_incremental_data = '/content/NYU/DS-data/100-increments'\n",
        "path_incremental_positives = os.path.join(path_incremental_data, 'positives')\n",
        "file_names = [file for file in os.listdir(path_incremental_positives) if file.endswith('csv')]\n",
        "print(*file_names, sep='\\n')\n",
        "\n",
        "df_data = []\n",
        "\n",
        "for filename in file_names:\n",
        "    df = pd.read_csv(os.path.join(path_incremental_positives, filename), index_col=None, header=0, sep=\",\")\n",
        "    df.columns = ['text', 'label']\n",
        "    df_data.append(df)\n",
        "\n",
        "data_positive = pd.concat(df_data, axis=0, ignore_index=True)\n",
        "#data_positive.rename(columns = {'sample':'text', 'category': 'label'}, inplace = True) #names of columns not consistent\n",
        "data_positive.dropna(axis=0, how=\"any\", inplace=True)\n",
        "data_positive['label'] = 'Hate-Speech'\n",
        "\n",
        "print(f\"Dreamspace annotated positives: {data_positive.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DreamSpace increments for negative examples"
      ],
      "metadata": {
        "id": "eEDK0VCnAXIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_incremental_negatives = os.path.join(path_incremental_data, 'negatives')\n",
        "file_names = [file for file in os.listdir(path_incremental_negatives) if file.endswith('csv')]\n",
        "print(*file_names, sep='\\n')\n",
        "\n",
        "df_data = []\n",
        "\n",
        "for filename in file_names:\n",
        "    df = pd.read_csv(os.path.join(path_incremental_negatives, filename), index_col=None, header=0, sep=\",\")\n",
        "    df.columns = ['text', 'label']\n",
        "    df_data.append(df)\n",
        "\n",
        "data_negative = pd.concat(df_data, axis=0, ignore_index=True)\n",
        "#data_positive.rename(columns = {'sample':'text', 'category': 'label'}, inplace = True) #names of columns not consistent\n",
        "data_negative.dropna(axis=0, how=\"any\", inplace=True)\n",
        "data_negative['label'] = 'Non-Hate-Speech'\n",
        "\n",
        "print(f\"Dreamspace annotated negatives: {data_negative.shape[0]}\")"
      ],
      "metadata": {
        "id": "mnwpj2z75s1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation CSV file"
      ],
      "metadata": {
        "id": "x7ludu1MAaAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tajGRs7tONK-"
      },
      "outputs": [],
      "source": [
        "data_validation = pd.read_csv(\"/content/NYU/tasks/task-4-language-transformer-models/data/validation-set.csv\", index_col=None, header=0)\n",
        "data_validation.rename(columns = {'sample':'text'}, inplace = True)\n",
        "data_validation['label'] = data_validation['label'].map(lambda x: 'Hate-Speech' if x == 'positive' else 'Non-Hate-Speech')\n",
        "\n",
        "print(f\"Dreamspace valdation positives: {data_validation.label.value_counts()[1]} ({data_validation.label.value_counts(normalize=True)[1]*100 :.2f}%)\")\n",
        "print(f\"Dreamspace valdation negatives: {data_validation.label.value_counts()[0]} ({data_validation.label.value_counts(normalize=True)[0]*100 :.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doccano data"
      ],
      "metadata": {
        "id": "9jZ14WHdAlqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ6tpNl5Oaq-"
      },
      "outputs": [],
      "source": [
        "doccano_annotated_file = '/content/NYU/tasks/task-2-data-annotation/data/doccano_annotated.csv'\n",
        "data_docano = pd.read_csv(doccano_annotated_file, index_col=0, header=0)\n",
        "data_docano['label'] = data_docano['label'].map(lambda x: 'Hate-Speech' if x == 'Positive' else 'Non-Hate-Speech')\n",
        "\n",
        "print(f\"Task 2 annotated positives: {data_docano.label.value_counts()[1]} ({data_docano.label.value_counts(normalize=True)[1]*100 :.2f}%)\")\n",
        "print(f\"Task 2 annotated negatives: {data_docano.label.value_counts()[0]} ({data_docano.label.value_counts(normalize=True)[0]*100 :.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homophobia dataset"
      ],
      "metadata": {
        "id": "zv7ijfEJAobC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI9zqI9MPdkz"
      },
      "outputs": [],
      "source": [
        "data_homophobia = pd.read_csv(\"/content/NYU/tasks/task-4-language-transformer-models/data/hate-speech-homophobia/hate-speech-homophobia.csv\", index_col=0, header=0)\n",
        "\n",
        "print(f\"homophobia dataset positives: {data_homophobia.label.value_counts()[1]} ({data_homophobia.label.value_counts(normalize=True)[1]*100 :.2f}%)\")\n",
        "print(f\"homophobia dataset negatives: {data_homophobia.label.value_counts()[0]} ({data_homophobia.label.value_counts(normalize=True)[0]*100 :.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XBZVbLpaz6_"
      },
      "source": [
        "For the test set, we consider the provided validation set and a small sample of the other datasets, given that the different datasets focus on different topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfDepUHabtJR"
      },
      "outputs": [],
      "source": [
        "train_homophobia, test_homophobia = train_test_split(\n",
        "    data_homophobia,\n",
        "    test_size=0.05,\n",
        "    random_state=1,\n",
        "    stratify=data_homophobia.label\n",
        ")\n",
        "\n",
        "train_docano, test_docano = train_test_split(\n",
        "    data_docano,\n",
        "    test_size=0.05,\n",
        "    random_state=1,\n",
        "    stratify=data_docano.label\n",
        ")\n",
        "\n",
        "train_positive, test_positive = train_test_split(\n",
        "    data_positive,\n",
        "    test_size=0.1,\n",
        "    random_state=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x__SCF-Eero0"
      },
      "outputs": [],
      "source": [
        "all_test = pd.concat([test_homophobia, test_docano, test_positive, data_negative, data_validation], axis=0, ignore_index=True)\n",
        "test_counts = all_test.label.value_counts(normalize=True)\n",
        "print(f\"Test positives: {all_test.shape[0]*test_counts[1]} ({test_counts[1]*100 :.2f})\")\n",
        "print(f\"Test negatives: {all_test.shape[0]*test_counts[0]} ({test_counts[0]*100 :.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5p1zSghhQ-J"
      },
      "outputs": [],
      "source": [
        "all_train = pd.concat([train_homophobia, train_docano, train_positive], axis=0, ignore_index=True)\n",
        "train_counts = all_train.label.value_counts(normalize=True)\n",
        "print(f\"Train positives: {all_train.shape[0]*train_counts[1]} ({train_counts[1]*100 :.2f})\")\n",
        "print(f\"Train negatives: {all_train.shape[0]*train_counts[0]} ({train_counts[0]*100 :.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVGPsAPGiW2c"
      },
      "outputs": [],
      "source": [
        "all_test.to_csv(\"/content/NYU/tasks/task-4-language-transformer-models/data/test.csv\")\n",
        "all_train.to_csv(\"/content/NYU/tasks/task-4-language-transformer-models/data/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaZ8vNRclhix"
      },
      "outputs": [],
      "source": [
        "!dvc status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJqBohA9lkAg"
      },
      "outputs": [],
      "source": [
        "!dvc add ../data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYyOEt47l1hq"
      },
      "outputs": [],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70XU-uXfl24X"
      },
      "outputs": [],
      "source": [
        "!git add ../data.dvc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rSuZprhl6OL"
      },
      "outputs": [],
      "source": [
        "!git commit -m \"Update datasets\"\n",
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPlX4Ww9lo7A"
      },
      "outputs": [],
      "source": [
        "!dvc push -r origin"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}